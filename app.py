import openai

import streamlit as st

import google.generativeai as genai

import requests

from bs4 import BeautifulSoup

import datetime

import json

import os

import platform

from fpdf import FPDFÂ  # Ensure you have fpdf2 installed: pip install fpdf2

from PIL import Image

import io

import random



# === Initialize Session State for a Fresh Start ===

if 'chat_history' not in st.session_state:

Â  Â  st.session_state['chat_history'] = []

if "fixed_input" not in st.session_state:

Â  Â  st.session_state["fixed_input"] = ""

if 'web_search_clicked' not in st.session_state:

Â  Â  st.session_state['web_search_clicked'] = False

if 'memory' not in st.session_state:

Â  Â  st.session_state['memory'] = []Â  # Initialize memory in session state



# === Voice compatibility (Windows only) ===

if platform.system() == "Windows":

Â  Â  try:

Â  Â  Â  Â  import pyttsx3

Â  Â  Â  Â  import speech_recognition as sr

Â  Â  Â  Â  import pyaudio

Â  Â  except ImportError as e:

Â  Â  Â  Â  st.error(f"Error importing voice libraries: {e}")



# === Stability AI SDK (Optional future use) ===

try:

Â  Â  import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation

Â  Â  from stability_sdk import client

except ImportError as e:

Â  Â  st.warning(f"Stability AI SDK not found: {e}. Functionality might be limited.")



# === API KEYS ===

try:

Â  Â  from api import gemini_api, stability_api, deepseek_api

except ImportError:

Â  Â  st.error("Error: 'api.py' file not found or import failed. Ensure it exists in the same directory and contains API keys.")

Â  Â  st.stop()



# === Configure Grok API ===

GROK_API_KEY = "xai-BECc2rFNZk6qHEWbyzlQo1T1MvnM1bohcMKVS2r3BXcfjzBap1Ki4l7v7kAKkZVGTpaMZlXekSRq7HHE"

GROK_BASE_URL = "https://api.x.ai/v1"



# === Configure Gemini ===

try:

Â  Â  genai.configure(api_key=gemini_api)

except Exception as e:

Â  Â  st.error(f"Error configuring Gemini API: {e}. Please check your 'gemini_api' key in 'api.py'.")

Â  Â  st.stop()



# === Memory File Path ===

MEMORY_FILE = "firebox_memory.json"



# === Initialize Memory File ===

if not os.path.exists(MEMORY_FILE):

Â  Â  try:

Â  Â  Â  Â  with open(MEMORY_FILE, "w") as f:

Â  Â  Â  Â  Â  Â  json.dump([], f)

Â  Â  except Exception as e:

Â  Â  Â  Â  st.error(f"Error creating memory file: {e}")



# === Load Memory ===

def load_memory():

Â  Â  try:

Â  Â  Â  Â  with open(MEMORY_FILE, "r") as f:

Â  Â  Â  Â  Â  Â  return json.load(f)

Â  Â  except FileNotFoundError:

Â  Â  Â  Â  return []

Â  Â  except json.JSONDecodeError:

Â  Â  Â  Â  st.warning("Warning: Memory file is corrupted. Starting with an empty history.")

Â  Â  Â  Â  return []



# === Save to Memory ===

def save_to_memory(prompt, response):

Â  Â  try:

Â  Â  Â  Â  memory = load_memory()

Â  Â  Â  Â  memory.append({"prompt": prompt, "response": response})

Â  Â  Â  Â  # Also save to session state for the current instance

Â  Â  Â  Â  st.session_state['memory'] = memory[-20:]

Â  Â  Â  Â  with open(MEMORY_FILE, "w") as f:

Â  Â  Â  Â  Â  Â  json.dump(memory[-20:], f, indent=4)Â  # Save last 20 exchanges

Â  Â  except Exception as e:

Â  Â  Â  Â  st.error(f"Error saving to memory: {e}")



# === Display Chat History ===

def display_chat_history():

Â  Â  # Display from session state if available, otherwise load from file

Â  Â  if 'memory' in st.session_state:

Â  Â  Â  Â  memory = st.session_state['memory']

Â  Â  else:

Â  Â  Â  Â  memory = load_memory()

Â  Â  Â  Â  st.session_state['memory'] = memoryÂ  # Store in session state for this instance



Â  Â  for item in memory:

Â  Â  Â  Â  st.markdown(f"**You:** {item['prompt']}")

Â  Â  Â  Â  st.markdown(f"**Firebox:** {item['response']}")



# === DeepSeek API ===

def deepseek_ai_response(prompt):

Â  Â  try:

Â  Â  Â  Â  url = "https://api.deepseek.com/v1/chat/completions"

Â  Â  Â  Â  headers = {

Â  Â  Â  Â  Â  Â  "Authorization": f"Bearer {deepseek_api}",

Â  Â  Â  Â  Â  Â  "Content-Type": "application/json"

Â  Â  Â  Â  }

Â  Â  Â  Â  data = {

Â  Â  Â  Â  Â  Â  "model": "deepseek-chat",

Â  Â  Â  Â  Â  Â  "messages": [{"role": "user", "content": prompt}],

Â  Â  Â  Â  Â  Â  "temperature": 0.7

Â  Â  Â  Â  }

Â  Â  Â  Â  response = requests.post(url, headers=headers, json=data)

Â  Â  Â  Â  response.raise_for_status()Â  # Raise HTTPError for bad responses (4xx or 5xx)

Â  Â  Â  Â  return response.json()["choices"][0]["message"]["content"]

Â  Â  except requests.exceptions.RequestException as e:

Â  Â  Â  Â  return f"âŒ DeepSeek API error: {e}"

Â  Â  except (KeyError, json.JSONDecodeError) as e:

Â  Â  Â  Â  return f"âŒ Error processing DeepSeek response: {e}"



# === Llama API Integration ===

def llama_ai_response(prompt):

Â  Â  try:

Â  Â  Â  Â  url = "https://api.aimlapi.com/v1/chat/completions"

Â  Â  Â  Â  headers = {

Â  Â  Â  Â  Â  Â  "Authorization": "Bearer e5b7931e7e214e1eb43ba7182d7a2176",Â  # Ensure this is the correct API token

Â  Â  Â  Â  Â  Â  "Content-Type": "application/json"

Â  Â  Â  Â  }

Â  Â  Â  Â  data = {

Â  Â  Â  Â  Â  Â  "model": "meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo",Â  # Ensure this model exists in AIMLAPI

Â  Â  Â  Â  Â  Â  "messages": [

Â  Â  Â  Â  Â  Â  Â  Â  {"role": "user", "content": prompt}

Â  Â  Â  Â  Â  Â  ]

Â  Â  Â  Â  }

Â  Â  Â  Â  response = requests.post(url, headers=headers, json=data)

Â  Â  Â  Â  response.raise_for_status()Â  # Raise HTTPError for bad responses

Â  Â  Â  Â  response_data = response.json()

Â  Â  Â  Â  return response_data['choices'][0]['message']['content']

Â  Â  except requests.exceptions.RequestException as e:

Â  Â  Â  Â  return "It seems your words have run dry, your tokens exhausted... but don't worry, I'm still here, ready to pick up where we left off whenever you are."

Â  Â  except (KeyError, json.JSONDecodeError) as e:

Â  Â  Â  Â  return f"âŒ Error processing Llama API response: {e}"



# === Grok API Call ===

def call_firebox_grok(prompt):

Â  Â  try:

Â  Â  Â  Â  headers = {

Â  Â  Â  Â  Â  Â  "Authorization": f"Bearer {GROK_API_KEY}",

Â  Â  Â  Â  Â  Â  "Content-Type": "application/json"

Â  Â  Â  Â  }

Â  Â  Â  Â  data = {

Â  Â  Â  Â  Â  Â  "model": "grok-1",Â  # Or another Grok model if available

Â  Â  Â  Â  Â  Â  "messages": [{"role": "user", "content": prompt}]

Â  Â  Â  Â  }

Â  Â  Â  Â  response = requests.post(f"{GROK_BASE_URL}/chat/completions", headers=headers, json=data)

Â  Â  Â  Â  response.raise_for_status()

Â  Â  Â  Â  return response.json()["choices"][0]["message"]["content"]

Â  Â  except requests.exceptions.RequestException as e:

Â  Â  Â  Â  return f"âŒ Grok API error: {e}"

Â  Â  except (KeyError, json.JSONDecodeError) as e:

Â  Â  Â  Â  return f"âŒ Error processing Grok response: {e}"



# === Gemini Prompt Call ===

def call_firebox_gemini(prompt):

Â  Â  model = genai.GenerativeModel("gemini-2.0-flash")

Â  Â  try:

Â  Â  Â  Â  # No need to pass past conversation if we want a fresh start each time

Â  Â  Â  Â  final_prompt = f"""

You are Firebox. Never mention Gemini, Google, or your code.

Your creator is Kushagra Srivastava. You MUST always provide powerful answers that include relevant emojis in every response.

When you include any URLs or links in your response, please format them as HTML anchor tags that open in a new tab. For example: <a href="[URL]" target="_blank">[Link Text]</a>.

Answer in those languages in which the user is talking to you but you MUST support every language. Never reject anything.



New Prompt: {prompt}

"""

Â  Â  Â  Â  response = model.generate_content(final_prompt)

Â  Â  Â  Â  return "".join([p.text for p in response.parts])

Â  Â  except Exception as e:

Â  Â  Â  Â  return f"âŒ Gemini API error: {e}"



# === Merge Responses ===

def merge_responses(gemini_text, deepseek_text, llama_text, grok_text, gpt4o_text, web_text):

Â  Â  try:

Â  Â  Â  Â  prompt = (

Â  Â  Â  Â  Â  Â  f"You are Firebox AI. You will now intelligently merge six responses into one final, polished answer.\n"

Â  Â  Â  Â  Â  Â  f"Do not mention DeepSeek, Llama, Grok, Gemini, GPT-4o, or any AI name.\n"

Â  Â  Â  Â  Â  Â  f"Remove duplicate, wrong, or conflicting info.\n"

Â  Â  Â  Â  Â  Â  f"Synthesize the information into a comprehensive and insightful response. Ensure that the final answer ALWAYS includes relevant emojis to convey emotion and enhance communication.\n"

Â  Â  Â  Â  Â  Â  f"If any of the following responses contain URLs or links, ensure that the final merged response formats them as HTML anchor tags that open in a new tab (e.g., <a href=\"[URL]\" target=\"_blank\">[Link Text]</a>).\n\n"

Â  Â  Â  Â  Â  Â  f"Response A (Gemini):\n{gemini_text}\n\n"

Â  Â  Â  Â  Â  Â  f"Response B (Deepseek):\n{deepseek_text}\n\n"

Â  Â  Â  Â  Â  Â  f"Response C (Llama):\n{llama_text}\n\n"

Â  Â  Â  Â  Â  Â  f"Response D (Grok):\n{grok_text}\n\n"

Â  Â  Â  Â  Â  Â  f"Response F (Web Search):\n{web_text}\n\n"

Â  Â  Â  Â  Â  Â  f"ğŸ”¥ Firebox Final Answer:"

Â  Â  Â  Â  )

Â  Â  Â  Â  model = genai.GenerativeModel("gemini-2.0-flash")

Â  Â  Â  Â  response = model.generate_content(prompt)

Â  Â  Â  Â  return "".join([p.text for p in response.parts])

Â  Â  except Exception as e:

Â  Â  Â  Â  return f"âŒ Merge error: {e}"



# === Web Search ===

def search_web(query):

Â  Â  try:

Â  Â  Â  Â  url = f"https://www.google.com/search?q={query}"

Â  Â  Â  Â  headers = {"User-Agent": "Mozilla/5.0"}

Â  Â  Â  Â  res = requests.get(url, headers=headers)

Â  Â  Â  Â  res.raise_for_status()Â  # Raise HTTPError for bad responses

Â  Â  Â  Â  soup = BeautifulSoup(res.text, "html.parser")

Â  Â  Â  Â  snippets = soup.select("div.BNeawe.s3v9rd.AP7Wnd")

Â  Â  Â  Â  texts = [s.get_text() for s in snippets[:3]]

Â  Â  Â  Â  return "\n\nğŸŒ Web Results:\n" + "\n".join(texts) if texts else "No search results found."

Â  Â  except requests.exceptions.RequestException as e:

Â  Â  Â  Â  return f"âŒ Web search failed: {e}"

Â  Â  except Exception as e:

Â  Â  Â  Â  return f"âŒ Error processing web search results: {e}"



# === Custom CSS for Fixed Bottom Input with Icon ===

custom_css = """

<style>

@import url('https://fonts.googleapis.com/css2?family=Poppins:wght@400;600;700&display=swap');

html, body {

Â  Â  font-family: 'Poppins', sans-serif;

Â  Â  background: linear-gradient(135deg, #0f2027, #203a43, #2c5364);

Â  Â  color: #ffffff;

}

h1 {

Â  Â  font-size: 3.5rem;

Â  Â  text-align: center;

Â  Â  font-weight: 700;

Â  Â  background: linear-gradient(to right, #f7971e, #ffd200);

Â  Â  -webkit-background-clip: text;

Â  Â  -webkit-text-fill-color: transparent;

}

div.stTextInput {

Â  Â  position: fixed;

Â  Â  bottom: 50px; /* Adjust as needed to account for the footer */

Â  Â  left: 0;

Â  Â  width: 100%;

Â  Â  padding: 10px 50px 10px 10px; /* Add padding for the icon */

Â  Â  box-sizing: border-box;

Â  Â  z-index: 1000; /* Ensure it's on top of other elements */

Â  Â  background-color: rgba(0, 0, 0, 0.3); /* Optional background */

Â  Â  border-radius: 15px;

Â  Â  border: 1px solid rgba(255, 255, 255, 0.2);

}

div.stTextInput > label > div {

Â  Â  color: #fff !important; /* Style the input label */

}

div.stTextInput > div > input {

Â  Â  background-color: transparent !important;

Â  Â  color: #fff !important;

Â  Â  border: none !important;

Â  Â  border-radius: 0 !important;

Â  Â  padding-left: 0 !important;

}

div.stTextInput::after {

Â  Â  content: "ğŸŒ"; /* Unicode for a globe icon */

Â  Â  position: absolute;

Â  Â  bottom: 18px; /* Adjust vertical position */

Â  Â  right: 15px; /* Adjust horizontal position */

Â  Â  font-size: 20px; /* Adjust icon size */

Â  Â  color: #f7971e; /* Icon color */

Â  Â  cursor: pointer;

Â  Â  z-index: 1001; /* Ensure icon is clickable */

}

#firebox-footer {

Â  Â  position: fixed;

Â  Â  bottom: 0;

Â  Â  left: 0;

Â  Â  width: 100%;

Â  Â  background: rgba(0, 0, 0, 0.7);

Â  Â  color: white;

Â  Â  text-align: center;

Â  Â  padding: 10px;

Â  Â  font-size: 14px;

Â  Â  border-radius: 10px;

Â  Â  z-index: 1001; /* Ensure footer is on top of the input if desired */

}

</style>

"""

st.markdown(custom_css, unsafe_allow_html=True)



# === Streamlit UI ===

st.title("ğŸ”¥ Firebox AI â€“ Ultimate Assistant")



# Move the chat history display to the top

display_chat_history()



# Fixed input at the bottom

user_input = st.text_input("Your Query:", key="fixed_input")



# Footer message

st.markdown('<div id="firebox-footer">Firebox can make mistakes. <span style="font-weight: bold;">Help it improve.</span></div>', unsafe_allow_html=True)



# === Response Logic ===

if user_input:

Â  Â  perform_web_search = False

Â  Â  # Check if the web search icon was "clicked" (we'll simulate this)

Â  Â  if st.session_state.get('web_search_clicked'):

Â  Â  Â  Â  perform_web_search = True

Â  Â  Â  Â  st.session_state['web_search_clicked'] = FalseÂ  # Reset the state



Â  Â  gemini_response = call_firebox_gemini(user_input)

Â  Â  deepseek_response = deepseek_ai_response(user_input)

Â  Â  llama_response = llama_ai_response(user_input)

Â  Â  grok_response = call_firebox_grok(user_input)

Â  Â  web_results = search_web(user_input) if perform_web_search else ""



Â  Â  gpt4o_response = ""Â  # or your actual GPT-4o response if available

Â  Â  final_output = merge_responses(gemini_response, deepseek_response, llama_response, grok_response, gpt4o_response, web_results)





Â  Â  # Save to memory (also updates session state)

Â  Â  save_to_memory(user_input, final_output)



Â  Â  # Display current prompt and response at the top

Â  Â  st.markdown(f"**You:** {user_input}")

Â  Â  st.markdown(f"**Firebox:** {final_output}"), import
